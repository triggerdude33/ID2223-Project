{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install package requirements and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knTg7_pldyws",
    "outputId": "df704386-549b-4375-ba6d-2dfab5ba3e92"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "import subprocess\n",
    "from retry_requests import retry\n",
    "from io import StringIO\n",
    "import hopsworks\n",
    "import great_expectations as ge\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment variables from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch former ski resorts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                name  year_closed  latitude  longitude\n",
      "0      2                       Splügen-Tambo         1998   46.5528     9.3217\n",
      "1      5            Oberammergau Lift System         1993   47.5988    11.0674\n",
      "4      7                        Monte Amiata         2015   42.8965    11.6111\n",
      "14    10                       Lomnicky Štít         2005   49.1954    20.2135\n",
      "18    17                 Sölkpass Ski Resort         1999   47.2956    13.7253\n",
      "..   ...                                 ...          ...       ...        ...\n",
      "353  492  Luchon-Superbagnères (Old Section)         2003   42.7903     0.5942\n",
      "354  503          Sperrin Mountains Ski Area         1995   54.7500    -7.0500\n",
      "358  564            Karawankenblick Ski Area         1993   46.6100    14.0200\n",
      "386  563              Le Chazelet Ski Resort         2001   45.0400     6.2900\n",
      "387  567                             La Tuca         1988   42.6356     0.7808\n",
      "\n",
      "[241 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# create supabase credential to authenticate towards endpoint\n",
    "result = subprocess.run(\n",
    "    [\"curl\", \"https://abandonedskitowns.com/get_key.php\"],    \n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    check=True\n",
    ")\n",
    "api_key = result.stdout\n",
    "\n",
    "# query the supabase instance for all ski resorts\n",
    "command = [\n",
    "    \"curl\",\n",
    "    \"https://uffrhqrrlipovcnrmgcz.supabase.co/rest/v1/main?select=*\",\n",
    "    \"-H\",\n",
    "    f\"apikey:{api_key}\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(\n",
    "    command,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    check=True\n",
    ")\n",
    "closed_resorts_json = result.stdout\n",
    "\n",
    "# convert closed resorts JSON to pandas object\n",
    "df_cr = pd.read_json(StringIO(closed_resorts_json))\n",
    "\n",
    "# filter out all resorts that don't have a closing date\n",
    "df_cr = df_cr[~df_cr[\"year_closed\"].isna()]\n",
    "df_cr = df_cr[~df_cr[\"year_closed\"].str.contains(\"Unknown\")]\n",
    "\n",
    "# filter out all resorts that specify decade instead of exact year\n",
    "df_cr = df_cr[~df_cr[\"year_closed\"].str.contains(\"s\")]\n",
    "\n",
    "# convert closed year to int\n",
    "df_cr['year_closed'] = pd.to_numeric(df_cr['year_closed'], downcast='integer', errors='coerce')\n",
    "\n",
    "# filter out all resorts which are not in Europe or North America\n",
    "df_cr = df_cr[(df_cr[\"area\"] == \"Europe\") | (df_cr[\"area\"] == \"North America\")]\n",
    "\n",
    "# filter out all columns except id, name, closing year, latitude, longitude\n",
    "df_cr = df_cr.filter(items=['id', 'name', 'year_closed', 'latitude', 'longitude'])\n",
    "\n",
    "print(df_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data validation rule for year\n",
    "Should not be lower than 1900 and not larger than the current year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"expectation_type\": \"expect_column_min_to_be_between\", \"kwargs\": {\"column\": \"year_closed\", \"min_value\": 1900, \"max_value\": 2025}, \"meta\": {}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closed_resort_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"closed_resort_expectation_suite\"\n",
    ")\n",
    "\n",
    "closed_resort_expectation_suite.add_expectation(\n",
    "    ge.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":\"year_closed\",\n",
    "            \"min_value\":1900,\n",
    "            \"max_value\":date.today().year\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log in to hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 17:00:11,153 INFO: Closing external client and cleaning up certificates.\n",
      "2025-12-23 17:00:11,154 INFO: Connection closed.\n",
      "2025-12-23 17:00:11,155 INFO: Initializing external client\n",
      "2025-12-23 17:00:11,155 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-12-23 17:00:12,115 WARNING: UserWarning: The installed hopsworks client version 4.6.0 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 17:00:13,208 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1271967\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send data to hopsworks feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/get feature store\n",
    "closed_resorts_fg = fs.get_or_create_feature_group(\n",
    "    name='former_resorts',\n",
    "    description='Ski resorts which have closed down for buisness',\n",
    "    version=1,\n",
    "    primary_key=['latitude', 'longitude'],\n",
    "    expectation_suite=closed_resort_expectation_suite\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 17:00:22,259 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1271967/fs/1258570/fg/1876342\n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Failed to write to delta table in external cluster. Make sure datanode load balancer has been setup on the cluster.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRestAPIError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hopsworks_common/core/variable_api.py:126\u001b[39m, in \u001b[36mVariableApi.get_loadbalancer_external_domain\u001b[39m\u001b[34m(self, service)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mloadbalancer_external_domain_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mservice\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hopsworks_common/core/variable_api.py:52\u001b[39m, in \u001b[36mVariableApi.get_variable\u001b[39m\u001b[34m(self, variable)\u001b[39m\n\u001b[32m     51\u001b[39m path_params = [\u001b[33m\"\u001b[39m\u001b[33mvariables\u001b[39m\u001b[33m\"\u001b[39m, variable]\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m domain = \u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m domain[\u001b[33m\"\u001b[39m\u001b[33msuccessMessage\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hopsworks_common/decorators.py:48\u001b[39m, in \u001b[36mconnected.<locals>.if_connected\u001b[39m\u001b[34m(inst, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hopsworks_common/client/base.py:186\u001b[39m, in \u001b[36mClient._send_request\u001b[39m\u001b[34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code // \u001b[32m100\u001b[39m != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.RestAPIError(url, response)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[31mRestAPIError\u001b[39m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/variables/loadbalancer_external_domain_datanode). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":100050,\"usrMsg\":\"Variable: loadbalancer_external_domain_datanodenot found\",\"errorMsg\":\"Requested variable not found\"}', error code: 100050, error msg: Requested variable not found, user msg: Variable: loadbalancer_external_domain_datanodenot found",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFeatureStoreException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hsfs/core/delta_engine.py:285\u001b[39m, in \u001b[36mDeltaEngine._setup_delta_rs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     datanode_ip = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loadbalancer_external_domain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatanode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     _logger.debug(\n\u001b[32m    289\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSetting HOPSFS_CLOUD_DATANODE_HOSTNAME_OVERRIDE to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatanode_ip\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hopsworks_common/core/variable_api.py:129\u001b[39m, in \u001b[36mVariableApi.get_loadbalancer_external_domain\u001b[39m\u001b[34m(self, service)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err.STATUS_CODE_NOT_FOUND:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\n\u001b[32m    130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClient could not get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLOADBALANCER_SERVICES[service]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service hostname from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloadbalancer_external_domain_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe variable is either not set or empty in Hopsworks cluster configuration.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFeatureStoreException\u001b[39m: Client could not get datanode service hostname from loadbalancer_external_domain_datanode. The variable is either not set or empty in Hopsworks cluster configuration.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFeatureStoreException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Insert Dataframe into feature group\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mclosed_resorts_fg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_cr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hsfs/feature_group.py:3454\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   3444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m   3445\u001b[39m     [\n\u001b[32m   3446\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._id,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3450\u001b[39m ):\n\u001b[32m   3451\u001b[39m     \u001b[38;5;66;03m# New delta FG allow for change data capture query\u001b[39;00m\n\u001b[32m   3452\u001b[39m     write_options[\u001b[33m\"\u001b[39m\u001b[33mdelta.enableChangeDataFeed\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3454\u001b[39m job, ge_report = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3455\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3457\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3458\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msave_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[38;5;66;03m# Compute stats in client if there is no backfill job:\u001b[39;00m\n\u001b[32m   3467\u001b[39m \u001b[38;5;66;03m# - spark engine: always compute in client\u001b[39;00m\n\u001b[32m   3468\u001b[39m \u001b[38;5;66;03m# - python engine: only compute if FG is offline only (no backfill job)\u001b[39;00m\n\u001b[32m   3469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine.get_type().startswith(\u001b[33m\"\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hsfs/core/feature_group_engine.py:247\u001b[39m, in \u001b[36mFeatureGroupEngine.insert\u001b[39m\u001b[34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28mself\u001b[39m._feature_group_api.delete_content(feature_group)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbulk_insert\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43monline_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43monline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    256\u001b[39m     ge_report,\n\u001b[32m    257\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hsfs/engine/python.py:1084\u001b[39m, in \u001b[36mEngine.save_dataframe\u001b[39m\u001b[34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[39m\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine.get_type() == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1083\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feature_group.time_travel_format == \u001b[33m\"\u001b[39m\u001b[33mDELTA\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1084\u001b[39m         delta_engine_instance = \u001b[43mdelta_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDeltaEngine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_store_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_store_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_store_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_store_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspark_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspark_session\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m         delta_engine_instance.save_delta_fg(\n\u001b[32m   1092\u001b[39m             dataframe,\n\u001b[32m   1093\u001b[39m             write_options=offline_write_options,\n\u001b[32m   1094\u001b[39m             validation_id=validation_id,\n\u001b[32m   1095\u001b[39m         )\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hsfs/core/delta_engine.py:76\u001b[39m, in \u001b[36mDeltaEngine.__init__\u001b[39m\u001b[34m(self, feature_store_id, feature_store_name, feature_group, spark_session, spark_context)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m._variable_api = variable_api.VariableApi()\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m._project_api = project_api.ProjectApi()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_delta_rs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.13/site-packages/hsfs/core/delta_engine.py:293\u001b[39m, in \u001b[36mDeltaEngine._setup_delta_rs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mHOPSFS_CLOUD_DATANODE_HOSTNAME_OVERRIDE\u001b[39m\u001b[33m\"\u001b[39m] = datanode_ip\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m FeatureStoreException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\n\u001b[32m    294\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to write to delta table in external cluster. Make sure datanode load balancer has been setup on the cluster.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    295\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    297\u001b[39m user_name = \u001b[38;5;28mself\u001b[39m._project_api.get_user_info().get(\u001b[33m\"\u001b[39m\u001b[33musername\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_name:\n",
      "\u001b[31mFeatureStoreException\u001b[39m: Failed to write to delta table in external cluster. Make sure datanode load balancer has been setup on the cluster."
     ]
    }
   ],
   "source": [
    "# Insert Dataframe into feature group\n",
    "closed_resorts_fg.insert(df_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADLGmgDTdqIe",
    "outputId": "f51e577b-7f64-4512-d08b-987bf61c087e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openmeteo_requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m cache_session = requests_cache.CachedSession(\u001b[33m'\u001b[39m\u001b[33m.cache\u001b[39m\u001b[33m'\u001b[39m, expire_after = \u001b[32m3600\u001b[39m)\n\u001b[32m      3\u001b[39m retry_session = retry(cache_session, retries = \u001b[32m5\u001b[39m, backoff_factor = \u001b[32m0.2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m openmeteo = \u001b[43mopenmeteo_requests\u001b[49m.Client(session = retry_session)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Make sure all required weather variables are listed here\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# The order of variables in hourly or daily is important to assign them correctly below\u001b[39;00m\n\u001b[32m      8\u001b[39m url = \u001b[33m\"\u001b[39m\u001b[33mhttps://api.open-meteo.com/v1/forecast\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'openmeteo_requests' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "##### CODE FROM OTHER PROJECT, WHOSE SOLE PURPOSE IS TO PROVIDE INSPIRATION\n",
    "##### PLEASE REMOVE ONCE NOTEBOOK IS DONE\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "\t\"longitude\": 15.382664,\n",
    "  \"latitude\": 65.389107,\n",
    "\t\"hourly\": [\"snow_depth\", \"snowfall\", \"temperature_2m\", \"surface_pressure\", \"cloud_cover\"],\n",
    "\t\"past_days\": 0,\n",
    "\t\"forecast_days\": 1,\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates: {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"Elevation: {response.Elevation()} m asl\")\n",
    "print(f\"Timezone difference to GMT+0: {response.UtcOffsetSeconds()}s\")\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_snow_depth = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_snowfall = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_temperature_2m = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_surface_pressure = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_cloud_cover = hourly.Variables(4).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend =  pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "\n",
    "hourly_data[\"snow_depth\"] = hourly_snow_depth\n",
    "hourly_data[\"snowfall\"] = hourly_snowfall\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "hourly_data[\"surface_pressure\"] = hourly_surface_pressure\n",
    "hourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "print(\"\\nHourly data\\n\", hourly_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
