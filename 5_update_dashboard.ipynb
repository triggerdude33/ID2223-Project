{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f8edd0a-a99b-4be9-bb03-4fcdc34d94fb",
   "metadata": {},
   "source": [
    "## Install package requirements and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1034ba5e-b425-4402-bbc2-f326dc3896f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt --quiet\n",
    "\n",
    "import openmeteo_requests\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "import subprocess\n",
    "from retry_requests import retry\n",
    "from io import StringIO\n",
    "import hopsworks\n",
    "import great_expectations as ge\n",
    "from datetime import date\n",
    "import json\n",
    "import time\n",
    "import statistics\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad18fc-a5a2-4a8b-a608-05d2ea7c3269",
   "metadata": {},
   "source": [
    "## Load environment variables from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9c213fc-bb41-4d72-9706-c635b26f239a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c04339-c239-4f76-bf4b-284c991fa115",
   "metadata": {},
   "source": [
    "## Connect to hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9f6b7075-7a6b-43bd-ad6a-3a2383914ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-05 14:48:12,962 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-05 14:48:12,964 INFO: Initializing external client\n",
      "2026-01-05 14:48:12,966 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-05 14:48:13,971 WARNING: UserWarning: The installed hopsworks client version 4.3.1 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-05 14:48:15,067 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1271967\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa3651ae-f2d2-4d3a-8e05-1b01baf9f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0053dd62-44fd-4dd2-a1e4-d5f293779637",
   "metadata": {},
   "source": [
    "## Get feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "46aeae3b-3bff-4d4f-a33d-4022df7415a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shutdown predictions feature group\n",
    "shutdown_predictions_fg = fs.get_feature_group(\n",
    "    name='shutdown_predictions',\n",
    "    version=1\n",
    ")\n",
    "\n",
    "# get closed resorts feature group\n",
    "open_resorts_fg = fs.get_feature_group(\n",
    "    name='current_resorts',\n",
    "    version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9d554d1f-21f1-4e4e-8460-d3e76fba4f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-05 15:41:48,233 ERROR: [Errno 2] Opening HDFS file '/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2026-01-05T15:41:48.232731775+01:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/matteus/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py\", line 394, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matteus/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py\", line 459, in read_query\n",
      "    return self._get_dataset(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matteus/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.11/site-packages/retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matteus/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.11/site-packages/retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matteus/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.11/site-packages/retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"/home/matteus/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.11/site-packages/retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matteus/src/KTH/ScaML/project/ID2223-Project/venv/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py\", line 445, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_flight.pyx\", line 1745, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow/_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/matteusbid2223_featurestore.db/shutdown_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2026-01-05T15:41:48.232731775+01:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ski_resort_id</th>\n",
       "      <th>will_shutdown</th>\n",
       "      <th>shutdown_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7752047</td>\n",
       "      <td>True</td>\n",
       "      <td>2028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1227308055</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ski_resort_id  will_shutdown  shutdown_year\n",
       "0        7752047           True           2028\n",
       "1     1227308055          False              0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psw_df = []\n",
    "\n",
    "try:\n",
    "    psw_df = shutdown_predictions_fg.read(dataframe_type=\"pandas\")\n",
    "except: # shutdown_predictions feature group is empty, create test data instead\n",
    "    psw_test_elem = {\n",
    "        \"ski_resort_id\": [7752047, 1227308055],\n",
    "        \"will_shutdown\": [True, False],\n",
    "        \"shutdown_year\": [2028, 0]\n",
    "    }\n",
    "    psw_df = pd.DataFrame(psw_test_elem)    \n",
    "\n",
    "psw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10c392d7-f748-4e4c-bf07-56f63d8275c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.96s) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ski_resort_id</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1226505097</td>\n",
       "      <td>Torgnon</td>\n",
       "      <td>45.814452</td>\n",
       "      <td>7.554285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>601135063</td>\n",
       "      <td>Font d'Urle Chaud Clapier</td>\n",
       "      <td>44.910152</td>\n",
       "      <td>5.323491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1254287966</td>\n",
       "      <td>Ristolas en Queyras</td>\n",
       "      <td>44.771783</td>\n",
       "      <td>6.960893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>601115623</td>\n",
       "      <td>Alpe Devero</td>\n",
       "      <td>46.307671</td>\n",
       "      <td>8.252052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7752047</td>\n",
       "      <td>San Martino di Castrozza - Passo Rolle</td>\n",
       "      <td>46.268927</td>\n",
       "      <td>11.792439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>45409595</td>\n",
       "      <td>Antagnod</td>\n",
       "      <td>45.822300</td>\n",
       "      <td>7.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>1227121146</td>\n",
       "      <td>Gitschenen – Isenthal</td>\n",
       "      <td>46.899355</td>\n",
       "      <td>8.501497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>601131935</td>\n",
       "      <td>Saint Luc - Chandolin</td>\n",
       "      <td>46.236511</td>\n",
       "      <td>7.625363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>642545662</td>\n",
       "      <td>Seefeld - Gschwandtkopf</td>\n",
       "      <td>47.317186</td>\n",
       "      <td>11.171586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>653944640</td>\n",
       "      <td>Schöneben - Belpiano</td>\n",
       "      <td>46.794364</td>\n",
       "      <td>10.503946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ski_resort_id                                    name   latitude  \\\n",
       "0       1226505097                                 Torgnon  45.814452   \n",
       "1        601135063               Font d'Urle Chaud Clapier  44.910152   \n",
       "2       1254287966                     Ristolas en Queyras  44.771783   \n",
       "3        601115623                             Alpe Devero  46.307671   \n",
       "4          7752047  San Martino di Castrozza - Passo Rolle  46.268927   \n",
       "..             ...                                     ...        ...   \n",
       "834       45409595                                Antagnod  45.822300   \n",
       "835     1227121146                   Gitschenen – Isenthal  46.899355   \n",
       "836      601131935                   Saint Luc - Chandolin  46.236511   \n",
       "837      642545662                 Seefeld - Gschwandtkopf  47.317186   \n",
       "838      653944640                    Schöneben - Belpiano  46.794364   \n",
       "\n",
       "     longitude  \n",
       "0     7.554285  \n",
       "1     5.323491  \n",
       "2     6.960893  \n",
       "3     8.252052  \n",
       "4    11.792439  \n",
       "..         ...  \n",
       "834   7.682800  \n",
       "835   8.501497  \n",
       "836   7.625363  \n",
       "837  11.171586  \n",
       "838  10.503946  \n",
       "\n",
       "[839 rows x 4 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_df = open_resorts_fg.read(dataframe_type=\"pandas\")\n",
    "or_df = or_df.rename(columns={\"id\": \"ski_resort_id\"})\n",
    "or_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a5bf0-b038-49fa-9a6d-b6deaa1be163",
   "metadata": {},
   "source": [
    "## Prepare array for writing to markdown table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "181c69f8-979e-4e87-a98f-3d74ca43dd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>will_shutdown</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>shutdown-year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ski_resort_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7752047</th>\n",
       "      <td>San Martino di Castrozza - Passo Rolle</td>\n",
       "      <td>True</td>\n",
       "      <td>46.268927</td>\n",
       "      <td>11.792439</td>\n",
       "      <td>2028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227308055</th>\n",
       "      <td>Euthal</td>\n",
       "      <td>False</td>\n",
       "      <td>47.091908</td>\n",
       "      <td>8.823070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  will_shutdown  \\\n",
       "ski_resort_id                                                          \n",
       "7752047        San Martino di Castrozza - Passo Rolle           True   \n",
       "1227308055                                     Euthal          False   \n",
       "\n",
       "                latitude  longitude  shutdown-year  \n",
       "ski_resort_id                                       \n",
       "7752047        46.268927  11.792439           2028  \n",
       "1227308055     47.091908   8.823070              0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_data_df = psw_df.set_index('ski_resort_id').join(or_df.set_index('ski_resort_id'))\n",
    "publish_data_df = publish_data_df.iloc[:, [2, 0, 3, 4, 1]]\n",
    "publish_data_df = publish_data_df.rename(columns={\"shutdown_year\": \"shutdown-year\"})\n",
    "publish_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1a17908-6257-481b-a52c-c4100166b0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>shutdown-year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ski_resort_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7752047</th>\n",
       "      <td>San Martino di Castrozza - Passo Rolle</td>\n",
       "      <td>46.268927</td>\n",
       "      <td>11.792439</td>\n",
       "      <td>2028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227308055</th>\n",
       "      <td>Euthal</td>\n",
       "      <td>47.091908</td>\n",
       "      <td>8.823070</td>\n",
       "      <td>&gt;15 years</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name   latitude  longitude  \\\n",
       "ski_resort_id                                                                 \n",
       "7752047        San Martino di Castrozza - Passo Rolle  46.268927  11.792439   \n",
       "1227308055                                     Euthal  47.091908   8.823070   \n",
       "\n",
       "              shutdown-year  \n",
       "ski_resort_id                \n",
       "7752047                2028  \n",
       "1227308055        >15 years  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reformat value for all resorts which are not predicted to close down\n",
    "# in the coming years\n",
    "publish_data_df = publish_data_df.astype({'shutdown-year': object})\n",
    "for i in range(0, len(publish_data_df.index)):\n",
    "    if not publish_data_df.iloc[i, 1]:\n",
    "        publish_data_df.iloc[i, 4] = \">15 years\"\n",
    "\n",
    "# drop columns which shouldn't be published to the github page\n",
    "publish_data_df = publish_data_df.drop(columns=['will_shutdown'])\n",
    "publish_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb4ce749-6f61-4590-9114-7864f39f0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown_table(data_df: pd.DataFrame):\n",
    "    header = list(data_df.columns.values)\n",
    "    rows = data_df.to_numpy()\n",
    "    header_row = \"| \" + \" | \".join(map(str, header)) + \" |\"\n",
    "    separator_row = \"| \" + \" | \".join(\"---\" for _ in header) + \" |\"\n",
    "\n",
    "    body_rows = [\n",
    "        \"| \" + \" | \".join(map(str, row)) + \" |\"\n",
    "        for row in rows\n",
    "    ]\n",
    "\n",
    "    return \"\\n\".join([header_row, separator_row, *body_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c578b93-b9b3-42ca-aaa5-6b76117285a6",
   "metadata": {},
   "source": [
    "## Update dashboard file\n",
    "\n",
    "Creates new table contents and writes them to the markdown dashboard file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c6d089bb-4104-425e-878e-198e3c797af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path().absolute()\n",
    "page_file_path = f\"{root_dir}/docs/index.md\"\n",
    "\n",
    "table_md = markdown_table(publish_data_df)\n",
    "file_contents = f\"\"\"# Predicted alpine ski resort close-down in the coming years\n",
    "\n",
    "Will your favorite alpine ski resort close in the next 15 years due to climate change?\n",
    "Take a look at our predictions in the table below\n",
    "\n",
    "{table_md}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open(page_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(file_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45291f-4ff7-468e-9c38-f6a28ae933d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
